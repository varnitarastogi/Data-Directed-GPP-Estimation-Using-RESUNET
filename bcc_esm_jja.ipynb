{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bcc_esm_jja.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVaninvG98j3"
      },
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "!conda install -c conda-forge iris"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLMHdoTn-JA_"
      },
      "source": [
        "import glob\n",
        "import iris\n",
        "from iris.util import unify_time_units\n",
        "import iris.quickplot as qplt\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import iris.analysis.cartography\n",
        "import iris.analysis.stats\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras  \n",
        "from tensorflow.keras.backend import int_shape     \n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Add, BatchNormalization, Input, Activation, Cropping2D, Concatenate,ZeroPadding2D\n",
        "import time\n",
        "import numpy.ma as ma\n",
        "import matplotlib.cm as mpl_cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9T3xCep-K_j"
      },
      "source": [
        "cubes_names=[]\n",
        "directories='/content/drive/MyDrive/U-Birmingham-gpp-data/jja/*BCC*.nc'\n",
        "\n",
        "for name in glob.glob(directories):\n",
        "    cubes_names.append(name)\n",
        "all_cubes = iris.load(cubes_names)\n",
        "print('Cubes:\\n',all_cubes)\n",
        "\n",
        "print('****************************************************************************************')\n",
        "  \n",
        "train_tas,train_pr,train_gpp,test_tas,test_pr,test_gpp,mask_train,mask_test=([] for i in range(8))\n",
        "train_years = iris.Constraint(year=lambda c: 1850 < c < 2200)\n",
        "test_years = iris.Constraint(year=lambda c: 2250 < c < 2300)\n",
        "\n",
        "for x in all_cubes:\n",
        "  #convert units\n",
        "    if(x._names[0]=='precipitation_flux'):\n",
        "       x.convert_units('kg m-2 months-1')\n",
        "    if(x._names[0]=='air_temperature'):\n",
        "       x.convert_units('celsius')\n",
        "    if(x._names[0]=='gross_primary_productivity_of_biomass_expressed_as_carbon'):\n",
        "       x.convert_units('kg m-2 months-1')\n",
        "    #scale data   \n",
        "    try:\n",
        "        \n",
        "        if(x._names[0]=='precipitation_flux'):\n",
        "            print('1')\n",
        "            data_temp=x.extract(train_years)\n",
        "            data_temp=(data_temp-data_temp.data.min())/(data_temp.data.max()-data_temp.data.min())\n",
        "            mask_train.append(data_temp.data._mask)\n",
        "            train_pr.append(data_temp.data.filled(0))\n",
        "            data_temp=x.extract(test_years)  \n",
        "            minimum_pr=data_temp.data.min()\n",
        "            maximum_pr=data_temp.data.max()  \n",
        "            mask_test.append(data_temp.data._mask)\n",
        "            data_temp=(data_temp-data_temp.data.min())/(data_temp.data.max()-data_temp.data.min())            \n",
        "            test_pr.append(data_temp.data.filled(0))\n",
        "        if(x._names[0]=='air_temperature'):\n",
        "            print('2')\n",
        "            data_temp=x.extract(train_years)\n",
        "            data_temp=(data_temp-data_temp.data.min())/(data_temp.data.max()-data_temp.data.min())\n",
        "            train_tas.append(data_temp.data.filled(0))\n",
        "            data_temp=x.extract(test_years)\n",
        "            minimum_tas=data_temp.data.min()\n",
        "            maximum_tas=data_temp.data.max()  \n",
        "            data_temp=(data_temp-data_temp.data.min())/(data_temp.data.max()-data_temp.data.min())\n",
        "            test_tas.append(data_temp.data.filled(0))\n",
        "        if(x._names[0]=='gross_primary_productivity_of_biomass_expressed_as_carbon'):\n",
        "            print('3')\n",
        "            data_temp=x.extract(train_years)\n",
        "            data_temp=(data_temp-data_temp.data.min())/(data_temp.data.max()-data_temp.data.min())\n",
        "            train_gpp.append(data_temp.data.filled(0))\n",
        "            data_temp=x.extract(test_years)\n",
        "            minimum_gpp=data_temp.data.min()\n",
        "            maximum_gpp=data_temp.data.max()        \n",
        "            data_temp=(data_temp-data_temp.data.min())/(data_temp.data.max()-data_temp.data.min())\n",
        "            test_gpp.append(data_temp.data.filled(0))\n",
        "         \n",
        "    except:\n",
        "            print(\"The variable does not exist\")\n",
        " \n",
        "train_pr=np.concatenate(train_pr)               \n",
        "train_tas=np.concatenate(train_tas)     \n",
        "train_gpp=np.concatenate(train_gpp)  \n",
        "mask_train=np.concatenate(mask_train)  \n",
        "test_tas=np.concatenate(test_tas)     \n",
        "test_pr=np.concatenate(test_pr)     \n",
        "test_gpp=np.concatenate(test_gpp)  \n",
        "mask_test=np.concatenate(mask_test)  \n",
        "train_data=np.stack([train_tas,train_pr],axis=-1)\n",
        "test_data=np.stack([test_tas,test_pr],axis=-1)\n",
        "\n",
        "#The following architecture has been taken from https://github.com/Nishanksingla/UNet-with-ResBlock/blob/master/resnet34_unet_model_2D.py\n",
        "\n",
        "def res_unet(filter_root, layers, n_class=1, input_size=(64,128,2), activation='relu', batch_norm=True, final_activation='linear'):\n",
        "    inputs = Input(input_size, name=\"InputLayer\")\n",
        "    x = inputs\n",
        "    # Dictionary for long connections\n",
        "    long_connection_store = {}\n",
        "    # x=ZeroPadding2D(padding=((3,3),(2,2)))(x)\n",
        "    # Down sampling\n",
        "    for i in range(layers):\n",
        "        out_channel = 2**i * filter_root\n",
        "        print(\"out_channel downsampling: {}\".format(out_channel))\n",
        "        # Residual/Skip connection\n",
        "        res = Conv2D(out_channel, kernel_size=1,padding='same', use_bias=False, name=\"Identity{}_1\".format(i))(x)#defining residual connection\n",
        "        # First Conv Block with Conv, BN and activation\n",
        "        conv1 = Conv2D(out_channel, kernel_size=3, padding='same', name=\"Conv{}_1\".format(i))(x)#first convolution\n",
        "        if batch_norm:\n",
        "            conv1 = BatchNormalization(name=\"BN{}_1\".format(i))(conv1)#normalize batch\n",
        "        act1 = Activation(activation, name=\"Act{}_1\".format(i))(conv1)#apply activation\n",
        "        # Second Conv block with Conv and BN only\n",
        "        conv2 = Conv2D(out_channel, kernel_size=3, padding='same', name=\"Conv{}_2\".format(i))(act1)#second convolution\n",
        "        if batch_norm:\n",
        "            conv2 = BatchNormalization(name=\"BN{}_2\".format(i))(conv2)#second normalization\n",
        "        resconnection = Add(name=\"Add{}_1\".format(i))([res, conv2])#add the first input of the block with the output of the block\n",
        "        act2 = Activation(activation, name=\"Act{}_2\".format(i))(resconnection)#apply activation\n",
        "        # Max pooling\n",
        "        if i < layers - 1:\n",
        "            long_connection_store[str(i)] = act2#storing the output of the block in a list\n",
        "            x = MaxPooling2D(padding='same', name=\"MaxPooling{}_1\".format(i))(act2)#downsampling\n",
        "        else:\n",
        "            x = act2\n",
        "    print(\"\\n\")\n",
        "    # Upsampling\n",
        "    for i in range(layers - 2, -1, -1):\n",
        "        print(\"i upsampling: {}\".format(i))\n",
        "        out_channel = 2**(i) * filter_root\n",
        "        print(\"out_channel upsampling: {}\".format(out_channel))\n",
        "        # long connection from down sampling path.\n",
        "        long_connection = long_connection_store[str(i)]\n",
        "        print(\"long_connection: {}\".format(long_connection))\n",
        "        up1 = UpSampling2D(name=\"UpSampling{}_1\".format(i))(x)\n",
        "        up_conv1 = Conv2D(out_channel, 2, activation='relu', padding='same', name=\"upsamplingConv{}_1\".format(i))(up1)\n",
        "        print(\"up_conv1: {}\".format(up_conv1))\n",
        "        crop_shape = get_crop_shape(tf.keras.backend.int_shape(up_conv1), int_shape(long_connection))\n",
        "        crop_connection = Cropping2D(cropping=crop_shape, name=\"upCrop{}_1\".format(i))(long_connection)\n",
        "        #  Concatenate.\n",
        "        up_conc = Concatenate(axis=-1, name=\"upConcatenate{}_1\".format(i))([up_conv1, crop_connection])\n",
        "        #  Convolutions\n",
        "        up_conv2 = Conv2D(out_channel, 3, padding='same', name=\"upConv{}_1\".format(i))(up_conc)\n",
        "        if batch_norm:\n",
        "            up_conv2 = BatchNormalization(name=\"upBN{}_1\".format(i))(up_conv2)\n",
        "        up_act1 = Activation(activation, name=\"upAct{}_1\".format(i))(up_conv2)\n",
        "        up_conv2 = Conv2D(out_channel, 3, padding='same', name=\"upConv{}_2\".format(i))(up_act1)\n",
        "        if batch_norm:\n",
        "            up_conv2 = BatchNormalization(name=\"upBN{}_2\".format(i))(up_conv2)\n",
        "        # Residual/Skip connection\n",
        "        res = Conv2D(out_channel, kernel_size=1,\n",
        "                     padding='same', use_bias=False, name=\"upIdentity{}_1\".format(i))(up_conc)\n",
        "        resconnection = Add(name=\"upAdd{}_1\".format(i))([res, up_conv2])\n",
        "        x = Activation(activation, name=\"upAct{}_2\".format(i))(resconnection)\n",
        "    # Final convolution\n",
        "    # x=Cropping2D(((3,3),(2,2)))(x)\n",
        "    output = Conv2D(n_class, 1, padding='same',activation=final_activation, name='output')(x)\n",
        "    return Model(inputs, outputs=output, name='Res-UNet')\n",
        "# This is not useful, when the input size is not the multiple of 2^layers.\n",
        "# Dimensions of activation map from down path gets smaller than acitvation map after up sampling operation.\n",
        "def get_crop_shape(target, source):\n",
        "    # source is coming from down sampling path.\n",
        "    # target is coming from up sampling operation.\n",
        "    source_height_width = np.array(source[1:-1])\n",
        "    target_height_widht = np.array(target[1:-1])\n",
        "    diff = (source_height_width - target_height_widht).tolist()\n",
        "    diff_tup = map(lambda x: (x//2, x//2) if x%2 == 0 else (x//2, x//2 + 1), diff)\n",
        "    return tuple(diff_tup)\n",
        "\n",
        "model=res_unet(32,4)\n",
        "model.summary()\n",
        "tf.keras.utils.plot_model(model)\n",
        "opt = keras.optimizers.Adam(learning_rate=1e-5)\n",
        "l1=tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.NONE)\n",
        "l2=tf.keras.losses.MeanSquaredError()\n",
        "model.compile(optimizer=opt,loss=l2,metrics=['accuracy'])\n",
        "\n",
        "#The following code has been taken from https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/\n",
        "callbacks = [\n",
        "     keras.callbacks.ModelCheckpoint(\n",
        "         filepath=\"/content/drive/MyDrive/varnita/bcc/jja/resnet/mse\",\n",
        "         save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
        "         monitor=\"val_loss\",\n",
        "         verbose=1,\n",
        "     ),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=5, min_lr=0.0000001),\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        # Stop training when `val_loss` is no longer improving\n",
        "        monitor=\"val_loss\",\n",
        "        restore_best_weights=True,\n",
        "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
        "        min_delta=1e-2,\n",
        "        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
        "        patience=30,\n",
        "        verbose=1,\n",
        "    ),\n",
        "    keras.callbacks.TensorBoard(\n",
        "    log_dir=\"/content/drive/MyDrive/varnita/bcc/jja/resnet/mse\",\n",
        "    histogram_freq=1,  # How often to log histogram visualizations\n",
        "    embeddings_freq=1,  # How often to log embedding visualizations\n",
        "    update_freq=\"epoch\",\n",
        "    )\n",
        "] \n",
        "model.fit({\"InputLayer\":train_data,},{\"output\": train_gpp },batch_size=8,epochs=200,callbacks=callbacks,validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbOs83b9-S7m"
      },
      "source": [
        "#visualizing the output from the model\n",
        "Model=model( {\"InputLayer\": test_data} )\n",
        "matrix=Model.numpy()\n",
        "\n",
        "num_reshape = np.reshape(matrix, (test_data.shape[0], 64,128))\n",
        "denorm_data = (num_reshape*(maximum_gpp-minimum_gpp))+minimum_gpp#denormalize data\n",
        "denorm_data = ma.masked_array(denorm_data,mask_test)# add mask to exclude ocean\n",
        "ctr=0\n",
        "brewer_cmap=mpl_cm.get_cmap('brewer_RdYlBu_11')\n",
        "for x in all_cubes:\n",
        "        if (x._names[0] == 'gross_primary_productivity_of_biomass_expressed_as_carbon'):\n",
        "            ground_truth = x.extract(test_years)\n",
        "            for x in data_temp.slices_over('time'):\n",
        "               qplt.contourf(x,cmap=brewer_cmap)\n",
        "               plt.title('GPP expressed as carbon for '+str(x.coord('season_year').points[0]))\n",
        "               plt.show()\n",
        "               plt.close(\"all\")\n",
        "            temp=ground_truth.shape[0]\n",
        "            estimation= ground_truth.copy()\n",
        "            estimation.data= denorm_data[ctr:(ctr+ground_truth.shape[0]),:,:]\n",
        "            ctr+=temp\n",
        "            for x in estimation.slices_over('time'):\n",
        "               qplt.contourf(x,cmap=brewer_cmap)\n",
        "               plt.title('GPP expressed as carbon for '+str(x.coord('season_year').points[0])+ ' estimation')\n",
        "               plt.show()\n",
        "               plt.close(\"all\")\n",
        "            difference=ground_truth-estimation\n",
        "            for x in difference.slices_over('time'):\n",
        "                qplt.contourf(x,cmap=brewer_cmap)\n",
        "                plt.title('Difference between ground truth and estimated GPP for year '+str(x.coord('season_year').points[0]))\n",
        "                plt.show()\n",
        "                plt.close(\"all\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JeDIc98_FPR"
      },
      "source": [
        "#for plotting graphs\n",
        "!pip install tensorboard\n",
        "%load_ext tensorboard\n",
        "import tensorflow as tf\n",
        "import datetime, os\n",
        "%tensorboard --logdir \"/content/drive/MyDrive/varnita/bcc/jja/resnet/mse\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
